{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "News_ARTICLE_DEEP_Neural.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmjmPicqi7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3e37418-f6e1-4d8a-971a-d11e488a46af"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text,sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QeLYlEyqi7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('BBC News Train.csv').fillna(' ')\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C-blGnkqi7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train_df['Text'].values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u_y19uEqi7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = {'tech': 1, 'business':2, 'politics': 3 ,'entertainment': 4, 'sport' : 5}\n",
        "y = train_df['Category'].values\n",
        "for i in range(len(y)):\n",
        "    y[i] = labels[y[i]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-4lSX6qi7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "faec6cfc-545a-45ec-a87b-ed062977c306"
      },
      "source": [
        "print(train_df.sample(10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      ArticleId                                               Text Category\n",
            "425        1159  disney settles disclosure charges walt disney ...        2\n",
            "1069        290  sydney to host north v south game sydney will ...        5\n",
            "790         940  dame julie pops in to see poppins mary poppins...        4\n",
            "928        1710  assembly ballot papers  missing  hundreds of b...        3\n",
            "1304        888  minister defends hunting ban law the law banni...        3\n",
            "72         1766  vickery upbeat about arm injury england prop p...        5\n",
            "1078       1963  south african car demand surges car manufactur...        2\n",
            "1163        909  british library gets wireless net visitors to ...        1\n",
            "37          805  yachvili savours france comeback france scrum-...        5\n",
            "1467        937  charvis set to lose fitness bid flanker colin ...        5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWrYETXxqi7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 20000\n",
        "max_text_length = 100"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAsOZmt4qi7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokenizer = text.Tokenizer(max_features)\n",
        "x_tokenizer.fit_on_texts(list(x))\n",
        "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
        "x_train_val = sequence.pad_sequences(x_tokenized,maxlen=max_text_length)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5odqYesAqi7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebbfbd9b-fa12-4e2a-a860-3b0fa663a8e4"
      },
      "source": [
        "embedding_dim = 100\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:],dtype ='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(f'Found {len(embeddings_index)} word vectors')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14673 word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUsGdn7hqi7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((max_features,embedding_dim))\n",
        "for word,index in x_tokenizer.word_index.items():\n",
        "    if (index>max_features-1):\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbM8GxRzqi74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "3a646227-af22-4baf-84f8-03e13dddde80"
      },
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "model = Sequential()\n",
        "'''model.add(Embedding(max_features,\n",
        "                   embedding_dim,\n",
        "                   embeddings_initializer = tf.keras.initializers.Constant(\n",
        "                   embedding_matrix),\n",
        "                   trainable = False))'''\n",
        "\n",
        "model.add(Embedding(max_features,100,input_length=100,trainable=True)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5,activation ='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 128)          117248    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 2,125,829\n",
            "Trainable params: 2,125,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZp7Rez-qi7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics =['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bvwhR_Tqi8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val=  train_test_split(x_train_val, y,\n",
        "                                                  test_size =0.10, random_state = 1)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIRtw9dqi8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "3de898e5-bec3-4c27-ae3c-650162fb109e"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "         batch_size = batch_size,\n",
        "         epochs = 10,\n",
        "         validation_data = (x_val,y_val))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1341 samples, validate on 149 samples\n",
            "Epoch 1/10\n",
            "1341/1341 [==============================] - 14s 11ms/step - loss: nan - accuracy: 0.0037 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1341/1341 [==============================] - 13s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1341/1341 [==============================] - 13s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1341/1341 [==============================] - 13s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1341/1341 [==============================] - 12s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fe33925c908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getrgvlsqi8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('BBC News Train.csv')\n",
        "x_test = test_df['Text'].values\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y9hL5R-qi8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
        "x_testing = sequence.pad_sequences(x_test_tokenized,maxlen = 100)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq-k5jvWqi8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77132f41-236a-428a-a1a9-e9a5b4d18f5b"
      },
      "source": [
        "y_testing = model.predict(x_testing, verbose=1,batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1490/1490 [==============================] - 1s 813us/step\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}